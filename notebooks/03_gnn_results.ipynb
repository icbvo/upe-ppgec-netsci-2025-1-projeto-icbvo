{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03 – GNN Results and Embedding Visualization\n",
    "\n",
    "This notebook loads the trained GNN link prediction model, inspects the final metrics, and visualizes the learned node embeddings on the collaboration network.\n",
    "\n",
    "It assumes that the training script `gnn/train_link_prediction_gnn.py` has already been executed and produced:\n",
    "\n",
    "- `../results/linkpred_gnn_best.pth` — checkpoint with the best encoder and predictor parameters.\n",
    "- `../results/linkpred_metrics.json` — JSON file with test AUC and test AP.\n",
    "- `../data/collaboration.edgelist.txt` — the collaboration edge list.\n",
    "\n",
    "We will:\n",
    "\n",
    "1. Load the graph and rebuild the same train/val/test split used for training (assuming the same seed).\n",
    "2. Load the best GNN model (encoder + predictor).\n",
    "3. Compute node embeddings with the encoder.\n",
    "4. Visualize embeddings using t-SNE, colored by node degree.\n",
    "5. Inspect final metrics (AUC, AP) from the JSON file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports\n",
    "\n",
    "We add the project root (`..`) to `sys.path` so that we can import the `gnn` package directly from this notebook.\n",
    "\n",
    "Directory layout assumed:\n",
    "\n",
    "```text\n",
    "upe-ppgec-netsci-2025-1-projeto-icbvo/\n",
    "├── gnn/\n",
    │   ├── __init__.py\n",
    │   ├── models.py\n",
    │   ├── utils.py\n",
    │   └── train_link_prediction_gnn.py\n",
    "├── data/\n",
    │   └── collaboration.edgelist.txt\n",
    "├── results/\n",
    │   ├── linkpred_gnn_best.pth\n",
    │   └── linkpred_metrics.json\n",
    "└── notebooks/\n",
    "    └── 03_gnn_results.ipynb\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from sklearn.manifold import TSNE\n",
    "\n",    
    "import json\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Make sure the project root is on sys.path\n",
    "PROJECT_ROOT = Path(\"..\").resolve()\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "print(\"Project root:\", PROJECT_ROOT)\n",
    "print(\"sys.path[0]:\", sys.path[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we import our own package and models/utilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gnn.models import get_encoder, LinkPredictor\n",
    "from gnn.utils import set_seed, print_graph_summary, save_metrics\n",
    "\n",
    "DATA_PATH = PROJECT_ROOT / \"data\" / \"collaboration.edgelist.txt\"\n",
    "RESULTS_DIR = PROJECT_ROOT / \"results\"\n",
    "CHECKPOINT_PATH = RESULTS_DIR / \"linkpred_gnn_best.pth\"\n",
    "METRICS_PATH = RESULTS_DIR / \"linkpred_metrics.json\"\n",
    "\n",
    "DATA_PATH, CHECKPOINT_PATH, METRICS_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Utility Functions (Splits and Features)\n",
    "\n",
    "We re-implement a minimal subset of the functions used in the training script to:\n",
    "\n",
    "- Load the edge list.\n",
    "- Build the same train/val/test splits (using the same seed and ratios).\n",
    "- Construct `edge_index` tensors.\n",
    "- Create degree-based node features.\n",
    "\n",
    "These must be consistent with `gnn/train_link_prediction_gnn.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_edge_list(path: Path) -> pd.DataFrame:\n",
    "    df = pd.read_csv(path, sep=r\"\\s+\", header=None, names=[\"u\", \"v\"])\n",
    "    df = df[df[\"u\"] != df[\"v\"]].copy()  # remove self-loops\n",
    "\n",
    "    # canonical undirected representation\n",
    "    u_min = np.minimum(df[\"u\"].values, df[\"v\"].values)\n",
    "    v_max = np.maximum(df[\"u\"].values, df[\"v\"].values)\n",
    "    df[\"u\"] = u_min\n",
    "    df[\"v\"] = v_max\n",
    "\n",
    "    df = df.drop_duplicates().reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def build_splits(df_edges: pd.DataFrame,\n",
    "                 train_ratio: float = 0.8,\n",
    "                 val_ratio: float = 0.1,\n",
    "                 seed: int = 42):\n",
    "    \"\"\"Reproduce the same random split used during training.\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    num_edges = len(df_edges)\n",
    "    perm = np.random.permutation(num_edges)\n",
    "    df_edges = df_edges.iloc[perm].reset_index(drop=True)\n",
    "\n",
    "    n_train = int(train_ratio * num_edges)\n",
    "    n_val = int(val_ratio * num_edges)\n",
    "\n",
    "    df_train = df_edges.iloc[:n_train].reset_index(drop=True)\n",
    "    df_val = df_edges.iloc[n_train:n_train + n_val].reset_index(drop=True)\n",
    "    df_test = df_edges.iloc[n_train + n_val:].reset_index(drop=True)\n",
    "\n",
    "    return df_train, df_val, df_test\n",
    "\n",
    "\n",
    "def df_to_edge_index(df: pd.DataFrame,\n",
    "                     num_nodes: int,\n",
    "                     undirected: bool = True) -> torch.Tensor:\n",
    "    u = torch.tensor(df[\"u\"].values, dtype=torch.long)\n",
    "    v = torch.tensor(df[\"v\"].values, dtype=torch.long)\n",
    "\n",
    "    if undirected:\n",
    "        edge_index = torch.stack([\n",
    "            torch.cat([u, v]),\n",
    "            torch.cat([v, u])\n",
    "        ], dim=0)\n",
    "    else:\n",
    "        edge_index = torch.stack([u, v], dim=0)\n",
    "    return edge_index\n",
    "\n",
    "\n",
    "def make_node_features(df_edges: pd.DataFrame, num_nodes: int) -> torch.Tensor:\n",
    "    deg = np.zeros(num_nodes, dtype=float)\n",
    "    for _, row in df_edges.iterrows():\n",
    "        deg[row[\"u\"]] += 1.0\n",
    "        deg[row[\"v\"]] += 1.0\n",
    "\n",
    "    deg = (deg - deg.mean()) / (deg.std() + 1e-9)\n",
    "    x = torch.tensor(deg[:, None], dtype=torch.float32)\n",
    "    return x\n",
    "\n",
    "print(\"Utility functions defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Graph and Rebuild Splits\n",
    "\n",
    "We load the edge list, compute the number of nodes, and rebuild the same splits used in the training script:\n",
    "\n",
    "- Train: 80%\n",
    "- Validation: 10%\n",
    "- Test: 10%\n",
    "\n",
    "We also print a summary of the graph using `print_graph_summary` from `gnn.utils`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)\n",
    "\n",
    "if not DATA_PATH.exists():\n",
    "    raise FileNotFoundError(f\"Edge list not found at {DATA_PATH}\")\n",
    "\n",
    "df_edges = load_edge_list(DATA_PATH)\n",
    "num_edges = len(df_edges)\n",
    "num_nodes = int(max(df_edges[\"u\"].max(), df_edges[\"v\"].max()) + 1)\n",
    "\n",
    "df_train, df_val, df_test = build_splits(df_edges, train_ratio=0.8, val_ratio=0.1, seed=42)\n",
    "\n",
    "print_graph_summary(\n",
    "    num_nodes=num_nodes,\n",
    "    num_edges=num_edges,\n",
    "    num_train_edges=len(df_train),\n",
    "    num_val_edges=len(df_val),\n",
    "    num_test_edges=len(df_test),\n",
    ")\n",
    "\n",
    "df_edges.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Build PyG Data Object and Load the Trained Model\n",
    "\n",
    "We:\n",
    "\n",
    "- Build the **training adjacency** (undirected) used by the GNN.\n",
    "- Build degree-based node features.\n",
    "- Instantiate the encoder and predictor with the same hyperparameters used during training.\n",
    "- Load the best checkpoint from `linkpred_gnn_best.pth`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gnn.models import get_encoder, LinkPredictor\n",
    "\n",
    "train_edge_index = df_to_edge_index(df_train, num_nodes=num_nodes, undirected=True)\n",
    "x = make_node_features(df_edges, num_nodes=num_nodes)\n",
    "\n",
    "data = Data(x=x, edge_index=train_edge_index)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "data = data.to(device)\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters used during training\n",
    "ENCODER_NAME = \"gcn\"   # or \"sage\" if you changed it\n",
    "EMB_DIM = 64\n",
    "HIDDEN_PRED = 64\n",
    "DROPOUT = 0.2\n",
    "\n",
    "encoder = get_encoder(\n",
    "    name=ENCODER_NAME,\n",
    "    in_channels=data.num_node_features,\n",
    "    hidden_channels=EMB_DIM,\n",
    "    out_channels=EMB_DIM,\n",
    "    dropout=DROPOUT,\n",
    ").to(device)\n",
    "\n",
    "predictor = LinkPredictor(emb_dim=EMB_DIM, hidden_dim=HIDDEN_PRED).to(device)\n",
    "\n",
    "if not CHECKPOINT_PATH.exists():\n",
    "    raise FileNotFoundError(f\"Checkpoint not found at {CHECKPOINT_PATH}\")\n",
    "\n",
    "checkpoint = torch.load(CHECKPOINT_PATH, map_location=device)\n",
    "encoder.load_state_dict(checkpoint[\"encoder\"])\n",
    "predictor.load_state_dict(checkpoint[\"predictor\"])\n",
    "\n",
    "encoder.eval()\n",
    "predictor.eval()\n",
    "\n",
    "print(\"Loaded checkpoint from:\", CHECKPOINT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Compute Node Embeddings\n",
    "\n",
    "We run the encoder on the full training graph to obtain node embeddings:\n",
    "\n",
    "- `z` has shape `[num_nodes, EMB_DIM]`.\n",
    "- We also compute node degrees to use as a simple scalar attribute in visualization.\n",
    "\n",
    "These embeddings can be used for:\n",
    "\n",
    "- Visualization (t-SNE, UMAP).\n",
    "- Downstream tasks (e.g., node classification, clustering).\n",
    "- Interpretation of authors' positions in the collaboration space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    z = encoder(data.x, data.edge_index)  # [num_nodes, EMB_DIM]\n",
    "\n",
    "z_np = z.cpu().numpy()\n",
    "z_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute degrees for coloring in the visualization\n",
    "deg = np.zeros(num_nodes, dtype=int)\n",
    "for _, row in df_edges.iterrows():\n",
    "    deg[row[\"u\"]] += 1\n",
    "    deg[row[\"v\"]] += 1\n",
    "\n",
    "deg[:10], deg.mean(), deg.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. t-SNE Visualization of Node Embeddings\n",
    "\n",
    "t-SNE is computationally expensive on very large graphs, so we may sample a subset of nodes for visualization.\n",
    "\n",
    "Steps:\n",
    "\n",
    "1. Optionally sample a subset of nodes (e.g., 2000 nodes).\n",
    "2. Run t-SNE on their embeddings.\n",
    "3. Color nodes according to their degree (e.g., log-degree).\n",
    "\n",
    "This gives a 2D projection of the GNN embedding space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nodes_total = z_np.shape[0]\n",
    "max_points = 2000  # for visualization\n",
    "\n",
    "if num_nodes_total > max_points:\n",
    "    np.random.seed(42)\n",
    "    sampled_idx = np.random.choice(num_nodes_total, size=max_points, replace=False)\n",
    "else:\n",
    "    sampled_idx = np.arange(num_nodes_total)\n",
    "\n",
    "z_sampled = z_np[sampled_idx]\n",
    "deg_sampled = deg[sampled_idx]\n",
    "\n",
    "z_sampled.shape, deg_sampled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(n_components=2, random_state=42, perplexity=30, init=\"pca\")\n",
    "z_2d = tsne.fit_transform(z_sampled)\n",
    "z_2d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "sc = plt.scatter(\n",
    "    z_2d[:, 0], z_2d[:, 1],\n",
    "    c=np.log1p(deg_sampled),\n",
    "    s=5,\n",
    "    cmap=\"viridis\",\n",
    "    alpha=0.7,\n",
    ")\n",
    "plt.colorbar(sc, label=\"log(1 + degree)\")\n",
    "plt.title(\"t-SNE of GNN Node Embeddings (Colored by Degree)\")\n",
    "plt.xlabel(\"t-SNE 1\")\n",
    "plt.ylabel(\"t-SNE 2\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Inspect Final Link Prediction Metrics\n",
    "\n",
    "We now load `linkpred_metrics.json` and inspect:\n",
    "\n",
    "- Test AUC\n",
    "- Test AP\n",
    "- Encoder type\n",
    "- Number of training epochs\n",
    "\n",
    "These values can be directly reported in the *Results* section of the research paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if METRICS_PATH.exists():\n",
    "    with open(METRICS_PATH, \"r\") as f:\n",
    "        metrics = json.load(f)\n",
    "    metrics\n",
    "else:\n",
    "    print(\"Metrics file not found at:\", METRICS_PATH)\n",
    "    metrics = None\n",
    "    metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If `metrics` is not `None`, you can now write something like:\n",
    "\n",
    "> \"The GNN-based link prediction model achieved a test AUC of X.XXX and a test Average Precision (AP) of Y.YYY using a GCN encoder with 64-dimensional node embeddings.\"\n",
    "\n",
    "in the *Results and Discussion* section of the paper.\n",
    "\n",
    "You may also compare these values with baseline heuristics (Common Neighbors, Jaccard, Adamic–Adar, Preferential Attachment) in a future notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
