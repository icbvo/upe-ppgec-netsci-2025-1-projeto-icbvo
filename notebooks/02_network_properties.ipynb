{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02 – Network Properties of the Collaboration Graph\n",
    "\n",
    "This notebook extends the exploratory analysis carried out in `01_exploration.ipynb` by computing and inspecting more advanced network properties of the collaboration graph.\n",
    "\n",
    "It focuses on the **largest connected component (LCC)** and analyzes:\n",
    "\n",
    "- Path length characteristics (average shortest path length, diameter approximation)\n",
    "- Assortativity by degree\n",
    "- Centrality measures (degree, betweenness, eigenvector)\n",
    "- Top nodes according to each centrality\n",
    "\n",
    "All results are saved in the `../results/` directory so they can be referenced in the research paper (Methods / Network Analysis section)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports\n",
    "\n",
    "We assume the same directory structure as before:\n",
    "\n",
    "```text\n",
    "upe-ppgec-netsci-2025-1-projeto-icbvo/\n",
    "├── data/\n",
    "│   └── collaboration.edgelist.txt\n",
    "├── notebooks/\n",
    "├── results/\n",
    "└── gnn/\n",
    "```\n",
    "\n",
    "We will load the edge list and build the graph again using NetworkX. We will also load the `graph_summary.json` generated in `01_exploration.ipynb` if available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "DATA_PATH = Path(\"../data/collaboration.edgelist.txt\")\n",
    "RESULTS_DIR = Path(\"../results\")\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "SUMMARY_PATH = RESULTS_DIR / \"graph_summary.json\"\n",
    "\n",
    "DATA_PATH, RESULTS_DIR, SUMMARY_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Edge List and Graph\n",
    "\n",
    "We load the edge list and construct an undirected NetworkX graph as before. This ensures that all metrics here are consistent with the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not DATA_PATH.exists():\n",
    "    raise FileNotFoundError(f\"Edge list file not found: {DATA_PATH}\")\n",
    "\n",
    "df_edges = pd.read_csv(DATA_PATH, sep=r\"\\s+\", header=None, names=[\"u\", \"v\"])\n",
    "df_edges.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.from_pandas_edgelist(df_edges, source=\"u\", target=\"v\")\n",
    "print(G)\n",
    "print(f\"Number of nodes: {G.number_of_nodes()}\")\n",
    "print(f\"Number of edges: {G.number_of_edges()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If available, we also load the basic summary computed in `01_exploration.ipynb` for reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SUMMARY_PATH.exists():\n",
    "    with open(SUMMARY_PATH, \"r\") as f:\n",
    "        summary_prev = json.load(f)\n",
    "    summary_prev\n",
    "else:\n",
    "    summary_prev = None\n",
    "    print(\"No previous summary found at\", SUMMARY_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Largest Connected Component (LCC)\n",
    "\n",
    "Many global metrics are best interpreted (and sometimes only defined) on connected graphs. We therefore focus on the **largest connected component (LCC)** of the collaboration network.\n",
    "\n",
    "This is also coherent with the idea of analyzing the giant component in complex networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "components = list(nx.connected_components(G))\n",
    "n_components = len(components)\n",
    "lcc_nodes = max(components, key=len)\n",
    "LCC = G.subgraph(lcc_nodes).copy()\n",
    "\n",
    "print(f\"Number of connected components: {n_components}\")\n",
    "print(f\"LCC number of nodes: {LCC.number_of_nodes()}\")\n",
    "print(f\"LCC number of edges: {LCC.number_of_edges()}\")\n",
    "print(f\"Fraction of nodes in LCC: {LCC.number_of_nodes() / G.number_of_nodes():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Path Length Characteristics in the LCC\n",
    "\n",
    "We approximate the distribution of shortest path lengths in the LCC by:\n",
    "\n",
    "- Computing exact average shortest path length on the LCC (if feasible), or\n",
    "- Sampling a subset of nodes if the LCC is very large.\n",
    "\n",
    "We also estimate the effective diameter (e.g., 90th percentile of shortest path lengths in the sample)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_lcc_nodes = LCC.number_of_nodes()\n",
    "print(f\"LCC size: {n_lcc_nodes} nodes\")\n",
    "\n",
    "# If the LCC is not too large, we can compute the average shortest path length directly.\n",
    "if n_lcc_nodes <= 5000:\n",
    "    avg_spl = nx.average_shortest_path_length(LCC)\n",
    "    print(f\"Average shortest path length (exact): {avg_spl:.4f}\")\n",
    "    sampled_path_lengths = None\n",
    "else:\n",
    "    print(\"LCC is large; approximating shortest path lengths by sampling nodes...\")\n",
    "    import random\n",
    "    random.seed(42)\n",
    "    sample_size = 500  # number of source nodes to sample\n",
    "    nodes_list = list(LCC.nodes())\n",
    "    sampled_nodes = random.sample(nodes_list, min(sample_size, n_lcc_nodes))\n",
    "\n",
    "    sampled_path_lengths = []\n",
    "    for s in sampled_nodes:\n",
    "        lengths = nx.single_source_shortest_path_length(LCC, s)\n",
    "        sampled_path_lengths.extend(list(lengths.values()))\n",
    "\n",
    "    sampled_path_lengths = np.array(sampled_path_lengths)\n",
    "    avg_spl = sampled_path_lengths.mean()\n",
    "    print(f\"Average shortest path length (sampled): {avg_spl:.4f}\")\n",
    "\n",
    "avg_spl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if LCC.number_of_nodes() <= 5000:\n",
    "    # For a smaller LCC, we can also compute the diameter exactly\n",
    "    try:\n",
    "        diameter = nx.diameter(LCC)\n",
    "        print(f\"Graph diameter (exact): {diameter}\")\n",
    "    except Exception as e:\n",
    "        print(\"Could not compute exact diameter:\", e)\n",
    "        diameter = None\n",
    "else:\n",
    "    diameter = None\n",
    "    if isinstance(sampled_path_lengths, np.ndarray):\n",
    "        p90 = np.percentile(sampled_path_lengths, 90)\n",
    "        print(f\"Approximate 90th percentile shortest path length (effective diameter): {p90:.4f}\")\n",
    "    else:\n",
    "        print(\"No sampled path lengths available for diameter approximation.\")\n",
    "\n",
    "diameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Assortativity by Degree\n",
    "\n",
    "We compute the **degree assortativity coefficient** of the LCC:\n",
    "\n",
    "- Positive assortativity indicates that high-degree nodes tend to connect to other high-degree nodes.\n",
    "- Negative assortativity indicates that high-degree nodes tend to connect to low-degree nodes.\n",
    "- Values close to zero suggest little or no degree correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degree_assortativity = nx.degree_assortativity_coefficient(LCC)\n",
    "print(f\"Degree assortativity coefficient (LCC): {degree_assortativity:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Centrality Measures\n",
    "\n",
    "We focus on three centrality measures computed on the LCC:\n",
    "\n",
    "- **Degree centrality**: normalized degree of each node.\n",
    "- **Betweenness centrality**: fraction of shortest paths that pass through the node.\n",
    "- **Eigenvector centrality**: importance of a node based on the importance of its neighbors.\n",
    "\n",
    "Note: Betweenness and eigenvector centralities can be computationally intensive on large graphs; in that case we may approximate or rely on tolerances/iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Degree centrality (fast)\n",
    "deg_centrality = nx.degree_centrality(LCC)\n",
    "\n",
    "# Betweenness centrality (may be expensive; we can sample nodes if needed)\n",
    "n_lcc_nodes = LCC.number_of_nodes()\n",
    "if n_lcc_nodes <= 5000:\n",
    "    betw_centrality = nx.betweenness_centrality(LCC, normalized=True)\n",
    "else:\n",
    "    # Approximate betweenness using k sampled nodes\n",
    "    import random\n",
    "    random.seed(42)\n",
    "    k = 500\n",
    "    sample_nodes = random.sample(list(LCC.nodes()), min(k, n_lcc_nodes))\n",
    "    betw_centrality = nx.betweenness_centrality(LCC, k=sample_nodes, normalized=True)\n",
    "\n",
    "# Eigenvector centrality (power iteration)\n",
    "try:\n",
    "    eig_centrality = nx.eigenvector_centrality(LCC, max_iter=1000, tol=1e-06)\n",
    "except Exception as e:\n",
    "    print(\"Eigenvector centrality did not converge:\", e)\n",
    "    eig_centrality = None\n",
    "\n",
    "len(deg_centrality), len(betw_centrality), (len(eig_centrality) if eig_centrality is not None else None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1. Top Nodes by Degree Centrality\n",
    "\n",
    "We list the top 10 nodes according to degree centrality in the LCC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_k_centrality(cent_dict, k=10):\n",
    "    return sorted(cent_dict.items(), key=lambda x: x[1], reverse=True)[:k]\n",
    "\n",
    "top10_deg = top_k_centrality(deg_centrality, k=10)\n",
    "top10_deg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2. Top Nodes by Betweenness Centrality\n",
    "\n",
    "Betweenness centrality highlights nodes that act as bridges in the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top10_betw = top_k_centrality(betw_centrality, k=10)\n",
    "top10_betw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3. Top Nodes by Eigenvector Centrality\n",
    "\n",
    "Eigenvector centrality captures nodes that are connected to other important nodes. If the computation did not converge, this may be `None`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if eig_centrality is not None:\n",
    "    top10_eig = top_k_centrality(eig_centrality, k=10)\n",
    "    top10_eig\n",
    "else:\n",
    "    print(\"Eigenvector centrality is not available.\")\n",
    "    top10_eig = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Export Centrality Results\n",
    "\n",
    "We aggregate centrality metrics into a single `pandas.DataFrame` and export them as a CSV file to the `../results/` directory.\n",
    "\n",
    "This file can later be used to:\n",
    "\n",
    "- Inspect specific nodes of interest\n",
    "- Compare centrality profiles\n",
    "- Support interpretations in the research paper (e.g., identifying hub authors or bridging authors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_lcc = list(LCC.nodes())\n",
    "\n",
    "df_centrality = pd.DataFrame({\n",
    "    \"node\": nodes_lcc,\n",
    "    \"degree_centrality\": [deg_centrality[n] for n in nodes_lcc],\n",
    "    \"betweenness_centrality\": [betw_centrality[n] for n in nodes_lcc],\n",
    "})\n",
    "\n",
    "if eig_centrality is not None:\n",
    "    df_centrality[\"eigenvector_centrality\"] = [eig_centrality[n] for n in nodes_lcc]\n",
    "else:\n",
    "    df_centrality[\"eigenvector_centrality\"] = np.nan\n",
    "\n",
    "centrality_csv_path = RESULTS_DIR / \"centrality_lcc.csv\"\n",
    "df_centrality.to_csv(centrality_csv_path, index=False)\n",
    "centrality_csv_path, df_centrality.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Final Summary\n",
    "\n",
    "We aggregate key properties of the LCC and save them in a JSON file named `network_properties_lcc.json` inside the `../results/` directory.\n",
    "\n",
    "These properties include:\n",
    "\n",
    "- LCC size (nodes, edges)\n",
    "- Fraction of nodes in the LCC\n",
    "- Average shortest path length (exact or sampled)\n",
    "- Diameter (exact or approximated) when available\n",
    "- Degree assortativity coefficient\n",
    "\n",
    "This file can be directly referenced in the *Network Description* section of the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "props_lcc = {\n",
    "    \"lcc_n_nodes\": int(LCC.number_of_nodes()),\n",
    "    \"lcc_n_edges\": int(LCC.number_of_edges()),\n",
    "    \"lcc_fraction_of_total_nodes\": float(LCC.number_of_nodes() / G.number_of_nodes()),\n",
    "    \"average_shortest_path_length\": float(avg_spl),\n",
    "    \"diameter_exact_or_none\": int(diameter) if diameter is not None else None,\n",
    "    \"degree_assortativity\": float(degree_assortativity),\n",
    "}\n",
    "\n",
    "props_path = RESULTS_DIR / \"network_properties_lcc.json\"\n",
    "with open(props_path, \"w\") as f:\n",
    "    json.dump(props_lcc, f, indent=4)\n",
    "\n",
    "print(f\"LCC properties saved to: {props_path}\")\n",
    "props_lcc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
